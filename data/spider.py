# -*- coding:utf-8 -*-# data by 58tongcheng# Author zzhimport requestsimport csvfrom bs4 import BeautifulSoupimport reclass spider:    def __init__(self, url):        self.url = url        self.page = 1        self.baseUrl = 'http://hu.58.com'        self.list=[]    #     处理url，组合rq and bs 返回结果    def __process(self, url, pattrem):        response = requests.get(url)        print response.status_code        if response.status_code==200:            print response.status_code            soup = BeautifulSoup(response.content, 'lxml')            resoult = soup.select(pattrem)            print resoult            return resoult        else:self.__process(url,pattrem)    # 返回所有区域后缀    def getPage(self):        url = self.url        areas = self.__process(url, 'body > div.mainbox > div.main > div.search_bd > dl.secitem.secitem_fist > dd > a')        # 删除区域中的不限        del areas[0]        return areas        # 递归获取所有租房信息的url  num为0 1 分别为个人房源和经纪人    def getAllPage(self, area, num):        url = self.baseUrl + area+str(num)+'/' + 'pn' + str(self.page)        print 'url='+url        a = self.__process(url, '#bottom_ad_li > div.pager > a.next')        print 'a'        print a        self.list.append([url,num])        if len(a) != 0:            self.page += 1            self.getAllPage(area,num)        else:return    #         获取一个页面的信息内容  0整租 1合租    def getOnePageInfo(self, url,source):        div = self.__process(url, "ul.listUl > li ")        if div!=[]:            del div[-1]            list=[]            for i in div:                if i.select('div.des > h2 > a')[0].text[26:28]==u'整租':                    type1=0                else:type1=1                size= i.select('div.des > p.room')[0].text[-4:-1].encode('utf-8')                price = i.select('div.listliright > div.money > b')[0].text.encode('utf-8')                address = i.select('div.des > p.add > a')[0].text.encode('utf-8')                source = source                item=[price,address,source,size,type1]                list.append(item)            return list        else:            print u'网络错误，获取失败'            list=[]            return list    def start(self):        area = self.getPage()        for o in area:            self.list=[]            infoList=[]            for i in range(2):                self.page=1                self.getAllPage(o['href'],i)            for n in self.list:                print n            #            #     infoList.extend(self.getOnePageInfo(n[0],n[1]))            #     self.writeCsv(o['href'][1:-7]+'.csv',infoList)    def writeCsv(self,fileName,data):        with open("csv/"+fileName,'wb') as csvFile:            writer = csv.writer(csvFile)            writer.writerow(['price','address','source','size','type'])            for i in data:                writer.writerow(i)    # def text(self):    #     for i in range(2):    #         self.page = 1    #         self.getAllPage('/jinqiaokfq/chuzu/',i)    #    #    #     for n in self.list:    #         print n[1], n[0]    #         infoList=[]    #         infoList.extend(sp.getOnePageInfo(n[0],n[1]))    #    #         self.writeCsv('gggg.csv', infoList)##sp = spider('http://hu.58.com/xinchengqu/chuzu/pn3/')# sp.writeCsv('text.csv',)# sp.writeCsv('text.csv',sp.getOnePageInfo('http://hu.58.com/xinchengqu/chuzu/pn3/'))sp.start()# sp.getOnePageInfo('http://hu.58.com/xinchengqu/chuzu/1/pn37',1)